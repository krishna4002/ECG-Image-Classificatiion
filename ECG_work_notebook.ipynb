{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1011472,"sourceType":"datasetVersion","datasetId":555422}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import ResNet18_Weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:43:20.103632Z","iopub.execute_input":"2025-05-17T16:43:20.103824Z","iopub.status.idle":"2025-05-17T16:43:27.861256Z","shell.execute_reply.started":"2025-05-17T16:43:20.103808Z","shell.execute_reply":"2025-05-17T16:43:27.860666Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n])\n\ntransform_val = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Load dataset\ntrain_data = datasets.ImageFolder('/kaggle/input/normalizedecgimagedata/Thesis3/train', transform=transform_train)\nval_data = datasets.ImageFolder('/kaggle/input/normalizedecgimagedata/Thesis3/validation', transform=transform_val)\ntest_data = datasets.ImageFolder('/kaggle/input/normalizedecgimagedata/Thesis3/test', transform=transform_val)\n\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=64)\ntest_loader = DataLoader(test_data, batch_size=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:48:03.479292Z","iopub.execute_input":"2025-05-17T16:48:03.479886Z","iopub.status.idle":"2025-05-17T16:48:16.032824Z","shell.execute_reply.started":"2025-05-17T16:48:03.479856Z","shell.execute_reply":"2025-05-17T16:48:16.032302Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass ResNetWithDropout(nn.Module):\n    def __init__(self, base_model, num_classes, dropout_rate=0.5):\n        super().__init__()\n        self.features = nn.Sequential(*list(base_model.children())[:-1])\n        self.dropout = nn.Dropout(dropout_rate)\n        self.classifier = nn.Linear(base_model.fc.in_features, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n# Load model\nfrom torchvision.models import resnet18, ResNet18_Weights\nbase_model = resnet18(weights=ResNet18_Weights.DEFAULT)\nmodel = ResNetWithDropout(base_model, num_classes=len(train_data.classes), dropout_rate=0.5)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:49:42.492352Z","iopub.execute_input":"2025-05-17T16:49:42.492625Z","iopub.status.idle":"2025-05-17T16:49:42.928325Z","shell.execute_reply.started":"2025-05-17T16:49:42.492604Z","shell.execute_reply":"2025-05-17T16:49:42.927765Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch.optim as optim\n\nnum_epochs = 3\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss, total_correct = 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_correct += (outputs.argmax(1) == labels).sum().item()\n\n    acc = 100 * total_correct / len(train_loader.dataset)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss:.4f}, Accuracy: {acc:.2f}%\")\n\n    # Validation\n    model.eval()\n    val_correct = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            val_correct += (outputs.argmax(1) == labels).sum().item()\n    val_acc = 100 * val_correct / len(val_loader.dataset)\n    print(f\"Validation Accuracy: {val_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:50:00.906806Z","iopub.execute_input":"2025-05-17T16:50:00.907564Z","iopub.status.idle":"2025-05-17T16:55:45.464392Z","shell.execute_reply.started":"2025-05-17T16:50:00.907533Z","shell.execute_reply":"2025-05-17T16:55:45.463620Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/3], Train Loss: 29.5760, Accuracy: 93.16%\nValidation Accuracy: 94.50%\nEpoch [2/3], Train Loss: 13.7549, Accuracy: 96.81%\nValidation Accuracy: 96.15%\nEpoch [3/3], Train Loss: 10.9033, Accuracy: 97.71%\nValidation Accuracy: 97.52%\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch.optim as optim\n\nnum_epochs = 4\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss, total_correct = 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_correct += (outputs.argmax(1) == labels).sum().item()\n\n    acc = 100 * total_correct / len(train_loader.dataset)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss:.4f}, Accuracy: {acc:.2f}%\")\n\n    # Validation\n    model.eval()\n    val_correct = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            val_correct += (outputs.argmax(1) == labels).sum().item()\n    val_acc = 100 * val_correct / len(val_loader.dataset)\n    print(f\"Validation Accuracy: {val_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T17:05:39.726389Z","iopub.execute_input":"2025-05-17T17:05:39.727099Z","iopub.status.idle":"2025-05-17T17:12:56.469613Z","shell.execute_reply.started":"2025-05-17T17:05:39.727074Z","shell.execute_reply":"2025-05-17T17:12:56.468969Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/4], Train Loss: 5.2902, Accuracy: 99.04%\nValidation Accuracy: 94.54%\nEpoch [2/4], Train Loss: 4.3482, Accuracy: 99.16%\nValidation Accuracy: 96.33%\nEpoch [3/4], Train Loss: 2.9524, Accuracy: 99.35%\nValidation Accuracy: 96.74%\nEpoch [4/4], Train Loss: 2.5992, Accuracy: 99.57%\nValidation Accuracy: 98.58%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T17:13:08.570620Z","iopub.execute_input":"2025-05-17T17:13:08.570884Z","iopub.status.idle":"2025-05-17T17:13:24.322146Z","shell.execute_reply.started":"2025-05-17T17:13:08.570864Z","shell.execute_reply":"2025-05-17T17:13:24.321444Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 98.35%\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"torch.save(model, 'ecg_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T17:15:13.378103Z","iopub.execute_input":"2025-05-17T17:15:13.378610Z","iopub.status.idle":"2025-05-17T17:15:13.456681Z","shell.execute_reply.started":"2025-05-17T17:15:13.378588Z","shell.execute_reply":"2025-05-17T17:15:13.455927Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}